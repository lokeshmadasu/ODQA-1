{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and Packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries \n",
    "import urllib.request # # urllib library used to retrieve the content from a url\n",
    "from bs4 import BeautifulSoup # bs4 used for pulling data from html or xml file with the help of a parser\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"QA_Pair_testing_file.txt\"\n",
    "f = open(test_file, \"rb\")\n",
    "lines = f.readlines()\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "for line in lines:\n",
    "    line = line.decode()\n",
    "    line = line.strip()\n",
    "    qa = line.split(\":::\")\n",
    "    questions.append(qa[1])\n",
    "    answers.append(qa[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Pre_tagged_labelled_data.txt\"\n",
    "f = open(file_name , 'rb')\n",
    "labels_list = []\n",
    "queries_list = []\n",
    "for each in f:\n",
    "#     print(each)\n",
    "    each = each.decode()\n",
    "    each = each.strip()\n",
    "#     print(each)\n",
    "    l = len(each.split(\":\"))\n",
    "    labels_list.append(str(each.split(\":\")[0]))\n",
    "    if(l==2):\n",
    "        queries_list.append(str(each.split(\":\")[1]))\n",
    "    else:\n",
    "        queries_list.append(':'.join(each.split(\":\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = {\n",
    "    \"Question\":queries_list,\n",
    "    \"Label\": labels_list\n",
    "}\n",
    "df = pd.DataFrame(data = obj )\n",
    "#print(df.head())\n",
    "# df.to_excel(\"final_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Question\"].values\n",
    "Y = df[\"Label\"].values\n",
    "X_train ,X_test ,Y_train ,Y_test = train_test_split(X,Y,test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data shape : (1073, 913) (1073,)\n",
      "Test_data shape  : (269, 913) (269,)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tf = vectorizer.fit_transform(X_train)\n",
    "X_test_tf = vectorizer.transform(X_test)\n",
    "\n",
    "# print(vectorizer.get_feature_names())\n",
    "print(\"Train_data shape :\",X_train_tf.shape , Y_train.shape)\n",
    "print(\"Test_data shape  :\",X_test_tf.shape , Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "Y_test = encoder.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.1, batch_size=71, hidden_layer_sizes=(100, 80, 50, 8),\n",
       "              random_state=4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier(activation='relu',solver='adam',hidden_layer_sizes=(100,80,50,8),random_state=4,alpha=0.1,batch_size=71)\n",
    "model.fit(X_train_tf,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = confusion_matrix(Y_test,test_pred)\n",
    "sns.heatmap(k , annot=True , fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DATE' 'LOCA' 'NUMB' 'ORGA' 'PERC' 'PERS' 'TIME']\n"
     ]
    }
   ],
   "source": [
    "print(encoder.inverse_transform([0,1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data : 0.6765799256505576\n"
     ]
    }
   ],
   "source": [
    "correctly_classified_count = 0\n",
    "for i in range(len(Y_test)):\n",
    "    if(Y_test[i]==test_pred[i]):\n",
    "        correctly_classified_count+=1\n",
    "print(\"Accuracy on test data : {0}\".format(correctly_classified_count/len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Final_model : </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Query_category(query):    \n",
    "    x_ = np.array([query])\n",
    "    x_tf = vectorizer.transform(x_)\n",
    "    prediction = model.predict(x_tf)\n",
    "    return encoder.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib2\n",
    "def query_translation(query):\n",
    "    agents = {'User-Agent':\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30)\"}\n",
    "    query = query.strip()\n",
    "    query = query.replace(\" \", \"+\")\n",
    "    link = \"https://translate.google.com/m?hl=en&sl=te&tl=en&ie=UTF-8&prev=_m&q={0}\".format(query)\n",
    "    #print(link)\n",
    "    try:\n",
    "        conten = requests.get(link, headers=agents, timeout=5).content\n",
    "        soup = BeautifulSoup(conten,'lxml')\n",
    "        tr = soup.find(\"div\",\"t0\")\n",
    "    except: #ConnectionError or read timout error\n",
    "        time.sleep(4)\n",
    "        try:\n",
    "            conten = requests.get(link, headers=agents,  timeout=5).content\n",
    "            soup = BeautifulSoup(conten,'lxml')\n",
    "            tr = soup.find(\"div\",\"t0\")\n",
    "        except Exception as e: #RemoteDisconnected\n",
    "            print(\"exception in query translation\",e)\n",
    "            return \"fail\" \n",
    "    return tr.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib2\n",
    "def ans_translation(ans):\n",
    "    agents = {'User-Agent':\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30)\"}\n",
    "    ans = ans.strip()\n",
    "    ans = ans.replace(\" \", \"+\")\n",
    "    link = \"https://translate.google.com/m?hl=en&sl=en&tl=te&ie=UTF-8&prev=_m&q={0}\".format(ans)\n",
    "    #print(link)\n",
    "    try:\n",
    "        conten = requests.get(link, headers=agents , timeout=5).content\n",
    "        soup = BeautifulSoup(conten,'lxml')\n",
    "        tr = soup.find(\"div\",\"t0\")\n",
    "    except: # ConnectionError or read time out error\n",
    "        time.sleep(4)\n",
    "        try:\n",
    "            conten = requests.get(link, headers=agents , timeout=5).content\n",
    "            soup = BeautifulSoup(conten,'lxml')\n",
    "            tr = soup.find(\"div\",\"t0\")\n",
    "        except Exception as e: #RemoteDisconnected\n",
    "            print(\"exception in answer translation\",e)\n",
    "            return \"UNABLE TO FIND\"\n",
    "    return tr.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_url_generation(query, search_engine):\n",
    "    # Function to generate url from a query\n",
    "\n",
    "    query = str(query).strip()\n",
    "    query = query.lower() # transform given query into lowercase\n",
    "    query = query.replace(' ', '+') # google/bing removes spaces and adds + symbol for url generation for a query\n",
    "\n",
    "    # URL generation\n",
    "    return 'https://' + search_engine + '/search?q=' + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_search(input_query , no_of_urls):\n",
    "    url = \"https://www.google.com/search?q=\"+input_query\n",
    "    main_url = url +\"&sxsrf=ALeKk03rVTWzT8epO1h0KE90ax9RrBcG5Q:1590835001614&ei=OTfSXv-PJfGH4-EPrs2fkAQ&start=&sa=N&ved=2ahUKEwi_kKuZstvpAhXxwzgGHa7mB0IQ8NMDegQIJBA1&safe=active\"\n",
    "    start_pos = main_url.find('start=')\n",
    "    p = start_pos+6\n",
    "    no_of_pages = math.ceil(no_of_urls/10)\n",
    "    count = 0\n",
    "    links_list = []\n",
    "    for i in range(0,no_of_pages):\n",
    "            try:\n",
    "                res = requests.get(url)\n",
    "            except Exception as e:\n",
    "                time.sleep(3)\n",
    "                try:\n",
    "                    res = requests.get(url)\n",
    "                except:\n",
    "                    time.sleep(6)\n",
    "                    res = requests.get(url)\n",
    "                print(\"exception {0} occured\".format(e))\n",
    "            #print(\"response status : {0} \".format(res.status_code))\n",
    "            while(res.status_code==429):\n",
    "                time.sleep(3)\n",
    "                res = requests.get(url)\n",
    "            soup = BeautifulSoup(res.text , \"lxml\")\n",
    "            g_results_div = soup.find_all('div' , attrs={'class':'ZINbbc xpd O9g5cc uUPGi'})\n",
    "            for each in g_results_div:\n",
    "                try:\n",
    "                    li = each.find('a' , href=True)\n",
    "                    link = li['href']\n",
    "                    link = str(link).split(\"&\")[0]\n",
    "                    link = link.replace(\"/url?q=\",\"\")\n",
    "                    if (link.startswith(\"https://\")):\n",
    "                        #print(link)\n",
    "                        links_list.append(link)\n",
    "                except:\n",
    "                    pass\n",
    "                #print(\"***************************************************\\n\")\n",
    "            count+=1\n",
    "            #print(\"page completed {0}\".format(count))\n",
    "            url = main_url[:p] + str(count*10) + main_url[p:]\n",
    "            #time.sleep(2)\n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bing_search(input_query , no_of_urls):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'}\n",
    "    input_query =  input_query.replace(\" \",\"+\")\n",
    "    url = \"https://www.bing.com/search?q=\"+input_query\n",
    "    main_url = url+\"&qs=HS&pq=who+is+the+pre&sc=8-14&cvid=AF7CFE3BF58449DB869F3D9A187C5222&sp=1&first=&FORM=PERE1\"\n",
    "    first_pos = main_url.find('first=') \n",
    "    p = first_pos + 6\n",
    "    #print(url)\n",
    "    no_of_pages = math.ceil(no_of_urls/10)\n",
    "    count = 0\n",
    "    bing_links = []\n",
    "    for each in range(0,no_of_pages):\n",
    "        res = requests.get(url,headers=headers)\n",
    "        #print(res.status_code)\n",
    "        while(res.status_code == 429):\n",
    "            res = requests.get(url,headers=headers)\n",
    "        soup = BeautifulSoup(res.content , \"lxml\")\n",
    "        #print(soup.prettify())\n",
    "        b_r_div = soup.find_all('li' ,  attrs = {'class':'b_algo'})\n",
    "        for each in b_r_div:\n",
    "            h  = each.find('h2')\n",
    "            link = h.find('a', href=True)\n",
    "            #print(link['href'])\n",
    "            bing_links.append(link['href'])\n",
    "        #print(len(bing_links))\n",
    "        count+=1\n",
    "        #print(\"page completed {0}\".format(count))\n",
    "        url =  main_url[:p] + str(count*10) + main_url[p:]\n",
    "        time.sleep(2)\n",
    "    return bing_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_ascii(text):\n",
    "    return  \"\".join([i if ord(i)<128 else \" \" for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_text_to_vec(text):\n",
    "    WORD = re.compile(r\"\\w+\")\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(vect1, vect2):\n",
    "    intersection = set(vect1.keys()) & set(vect2.keys())\n",
    "    nume = sum([vect1[x] * vect2[x] for x in intersection])\n",
    "    \n",
    "    sum1 = sum([vect1[x]**2 for x in vect1.keys()])\n",
    "    sum2 = sum([vect2[x]**2 for x in vect2.keys()])\n",
    "    denom = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    #print(intersection)\n",
    "    if not denom:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return nume/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(links_list):\n",
    "    full_text = \" \"\n",
    "    for each in links_list:\n",
    "        print(each)\n",
    "        text = \" \"\n",
    "        if(each.endswith(\".pdf\") or each.endswith(\".ppt\")):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                try:\n",
    "                    response = requests.get(each , timeout=5)\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    time.sleep(2)\n",
    "                    response = requests.get(each , timeout=5)\n",
    "                print(\"extract_data response code : {0}\".format(response.status_code))\n",
    "                each_soup = BeautifulSoup(response.content,'html.parser')\n",
    "                #print(each_soup)\n",
    "                for p in each_soup.find_all(\"p\"):\n",
    "                    text+= p.text + \" \"\n",
    "                full_text+=text\n",
    "            except Exception as e:\n",
    "                print(\"exception\",e)\n",
    "                print(\"{0} url max retries\".format(each))\n",
    "            time.sleep(2)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenization(full_text):\n",
    "    \n",
    "    ## sent-tokenization\n",
    "    sent_tok = re.split('[.?:;]+',full_text)\n",
    "    \n",
    "    return sent_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "stopwords_list= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_preprocess(x):\n",
    "    x = non_ascii(x)\n",
    "    x = str(x).strip()\n",
    "    x = x.replace(\"\\n\",\" \")\n",
    "    x = x.replace(\"\\t\",\" \")\n",
    "    x = x.replace(\"\\r\",\" \")\n",
    "    res = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x)   ### Removes '[anything], (anytext)'\n",
    "    s = res.replace(\"_\",\" \")\n",
    "    s = res.replace(\"-\",\" \")\n",
    "    s = s.replace(\"!\",\" \")\n",
    "    s = s.replace(\"?\",\" \")\n",
    "    s = s.replace(\"\\\\\",\" \")\n",
    "    s = s.replace(\"(\",\" \")\n",
    "    s = s.replace(\")\",\" \")\n",
    "    s = s.replace(\"[\",\" \")\n",
    "    s = s.replace(\"]\",\" \")\n",
    "    s = s.strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_preprocess(query_cat,best_answers):\n",
    "    final_answer_dict = {}\n",
    "    if query_cat==\"NUMB\":\n",
    "        #best_answers = {\"six\":1,\"6\":3,\"233\":5}\n",
    "        for key,value in best_answers.items():\n",
    "            if(key==\"one\"):\n",
    "                key='1'\n",
    "            elif(key==\"two\"):\n",
    "                key = '2'\n",
    "            elif(key==\"three\"):\n",
    "                key = '3'\n",
    "            elif(key==\"four\"):\n",
    "                key = '4'\n",
    "            elif(key==\"five\"):\n",
    "                key = '5'\n",
    "            elif(key==\"six\"):\n",
    "                key = \"6\"\n",
    "            elif(key==\"seven\"):\n",
    "                key = '7'\n",
    "            elif(key==\"eight\"):\n",
    "                key = '8'\n",
    "            elif(key==\"nine\"):\n",
    "                key = '9'\n",
    "            elif(key==\"zero\"):\n",
    "                key = '0'\n",
    "            if key not in final_answer_dict:\n",
    "                final_answer_dict[key] = value\n",
    "            else:\n",
    "                final_answer_dict[key]+=value\n",
    "    return final_answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort(s, n): \n",
    "    for i in range(1, n): \n",
    "        temp = s[i] \n",
    "        j = i - 1\n",
    "        while j >= 0 and len(temp) < len(s[j]): \n",
    "            s[j + 1] = s[j] \n",
    "            j -= 1\n",
    "\n",
    "        s[j + 1] = temp \n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_answer_processing_for_per(best_answers):\n",
    "    new_t1 = []\n",
    "    t1 = best_answers\n",
    "    for each in t1:\n",
    "        new_t1.append((each[0].lower(),each[1]))\n",
    "    new_t1 = (dict(new_t1))\n",
    "\n",
    "    #print(\"*************************************************\")\n",
    "    t1_keys = []\n",
    "    for each in t1:\n",
    "        t1_keys.append(each[0].lower())\n",
    "    t1_keys = sorted(list(set(t1_keys)))\n",
    "\n",
    "    #print(\"*************************************************\")\n",
    "\n",
    "    arr = t1_keys\n",
    "    n = len(arr)  \n",
    "    sorted_array = (sort(arr, n))\n",
    "\n",
    "    #print(\"**************************************************\")\n",
    "\n",
    "    unique = sorted_array\n",
    "    n = 0\n",
    "    for i in range(len(unique)):\n",
    "        n = i+1\n",
    "        for j in range(n,len(unique)):\n",
    "            a = unique[i]\n",
    "            b = unique[j]\n",
    "            v1 = from_text_to_vec(a)\n",
    "            v2 = from_text_to_vec(b)\n",
    "            cos = get_cosine(v1,v2)\n",
    "            if cos > 0:\n",
    "                #print({a:b})\n",
    "                #print(cos)\n",
    "                a_new = a.replace(\" \",\"\")\n",
    "                b_new = b.replace(\" \",\"\")\n",
    "                if len(a) < len(b):\n",
    "                    max_st = b\n",
    "                    min_st = a\n",
    "                else:\n",
    "                    max_st = a\n",
    "                    min_st = b\n",
    "                if (a in b) or (b in a):\n",
    "                    try:\n",
    "                        new_t1[max_st] += abs(new_t1[max_st] - new_t1[min_st])\n",
    "                        del new_t1[min_st]\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                elif(cos > 0.5):\n",
    "                    try:\n",
    "                        new_t1[max_st] += new_t1[min_st]\n",
    "                        del new_t1[min_st]\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                elif(a_new==b_new):\n",
    "                    new_t1[max_st] += new_t1[min_st]\n",
    "                    del new_t1[min_st]\n",
    "                break\n",
    "    return new_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def ner_spacy(input_text,trans_query):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    ners = nlp(input_text)\n",
    "    \n",
    "    Ner_Dic = {}\n",
    "    Ner_Entites_Dict = {}\n",
    "    unique_ners = set([])\n",
    "    \n",
    "    query_word = re.compile(r\"\\w+\")\n",
    "    query_words = query_word.findall(trans_query)\n",
    "    \n",
    "    #print(query_words)\n",
    "    for ent in ners.ents:\n",
    "        if ent.text not in Ner_Dic:\n",
    "            Ner_Dic[ent.text] = ent.label_\n",
    "    #print(Ner_Dic)\n",
    "    unique_ners = [\"PERSON\",\"NORP\",\"FAC\",\"ORG\",\"GPE\",\"LOC\",\"PRODUCT\",\"EVENT\",\"WORK_OF_ART\",\"LAW\",\"LANGUAGE\",\"DATE\",\"TIME\",\"PERCENT\",\"MONEY\",\"QUANTITY\",\"ORDINAL\",\"CARDINAL\"]\n",
    "    new = []\n",
    "    for i in unique_ners:\n",
    "        l = []\n",
    "        for j in Ner_Dic:\n",
    "            if ((Ner_Dic[j] == i) and (j.lower() not in query_words)):\n",
    "                l.append(j)\n",
    "            else:\n",
    "                Ner_Entites_Dict[str(i)] = []\n",
    "        Ner_Entites_Dict[str(i)] = l\n",
    "    \n",
    "    return Ner_Entites_Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_final(top_sentences, query_cat,trans_query , trans_query_for_checking):\n",
    "        best_answers = {}\n",
    "        ner_sent = \". \".join(top_sentences)\n",
    "        ner_sent = non_ascii(ner_sent)\n",
    "        #print(ner_sent)\n",
    "        possible_entities = ner_spacy(ner_sent,trans_query)\n",
    "        #print(\"possible_entities \",possible_entities)\n",
    "        \n",
    "        ####  Named Entity Recognition ends ##############\n",
    "\n",
    "        #### Extracting the possible answers  ###########\n",
    "        possible_answers = []\n",
    "        if query_cat == \"LOCA\":\n",
    "            possible_answers = possible_entities[\"GPE\"] + possible_entities[\"LOC\"]\n",
    "        elif query_cat == \"PERS\":\n",
    "            possible_answers = possible_entities[\"PERSON\"]\n",
    "        elif query_cat == \"DATE\":\n",
    "            possible_answers = possible_entities[\"DATE\"]\n",
    "        elif query_cat == \"ORGA\":\n",
    "            possible_answers = possible_entities[\"ORG\"]\n",
    "        elif query_cat == \"PERC\":\n",
    "            possible_answers = possible_entities[\"PERCENT\"]\n",
    "        elif query_cat == \"TIME\":\n",
    "            possible_answers = possible_entities[\"TIME\"]\n",
    "        elif query_cat == \"NUMB\":\n",
    "            possible_answers = possible_entities[\"CARDINAL\"] + possible_entities[\"QUANTITY\"]\n",
    "        \n",
    "        \n",
    "        #print(\"possible_answers\",possible_answers)\n",
    "        ##### Finding the best answer in possible answer ######\n",
    "        ner_sent = ner_sent.lower()\n",
    "        for answer in possible_answers:\n",
    "            answer = answer.lower()\n",
    "            if answer not in trans_query:\n",
    "                best_answers[answer] = ner_sent.count(answer)    \n",
    "    \n",
    "        #print(best_answers)\n",
    "        if(query_cat==\"NUMB\"):\n",
    "            best_answers = ans_preprocess(query_cat , best_answers)\n",
    "            best_answers_sorted = sorted(best_answers.items(), key=lambda x:x[1], reverse=True)\n",
    "            \n",
    "        elif(query_cat == \"PERS\"):\n",
    "            \n",
    "            best_answers_sorted = sorted(best_answers.items(), key=lambda x:x[1], reverse=True)\n",
    "            best_answers = best_answer_processing_for_per(best_answers_sorted)\n",
    "            best_answers_sorted = sorted(best_answers.items(), key=lambda x:x[1], reverse=True)\n",
    "            \n",
    "        else:\n",
    "            best_answers_sorted = sorted(best_answers.items(), key=lambda x:x[1], reverse=True)\n",
    "            \n",
    "        #print(\"best answers sorted{0}\".format(best_answers_sorted))\n",
    "        if len(best_answers_sorted) == 0:\n",
    "            answer = \"UNABLE TO FIND\"\n",
    "        else:\n",
    "            if (best_answers_sorted[0][0]==\"#\"):\n",
    "                return \"#\"\n",
    "            else:\n",
    "                for i in range(len(best_answers_sorted)):\n",
    "                    ans = best_answers_sorted[i][0]\n",
    "                    ans = ans.split()\n",
    "                    ans = \"\".join(ans)\n",
    "                    if ans not in trans_query:\n",
    "                        if ans not in trans_query_for_checking:\n",
    "                            answer = ans_translation(best_answers_sorted[i][0])\n",
    "                            break\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_preprocess_loca():\n",
    "    possible_answers_modified=[]\n",
    "    for count in range(len(sent_count)):\n",
    "        possible_answers=[]\n",
    "        for i in possible_answers_list[count]:\n",
    "            try:\n",
    "                if (re.search(i.lower(),query_english)) == None:\n",
    "                    possible_answers.append(i)\n",
    "            except:\n",
    "                None\n",
    "\n",
    "            if possible_answers!=[]:\n",
    "                try:\n",
    "                    for j in possible_answers:\n",
    "                            if (re.search('state',query_english)!=None or 'where' in query_words) and (j.lower() in ['india','indian',\"india's\"]):\n",
    "                                possible_answers.remove(i)\n",
    "                except:\n",
    "                    None\n",
    "            \n",
    "        possible_answers_modified.append(possible_answers)\n",
    "    \n",
    "    possible_answers_list=possible_answers_modified\n",
    "    \n",
    "    possible_answers_modified_1=[]\n",
    "    for count in range(len(sent_count)):\n",
    "        possible_answers=[]\n",
    "        for i in possible_answers_list[count]:\n",
    "            try:\n",
    "                if (re.search(i.lower(),query_english)) == None:\n",
    "                    possible_answers.append(i)\n",
    "            except:\n",
    "                None\n",
    "\n",
    "            if possible_answers!=[]:\n",
    "                try:\n",
    "                    for j in possible_answers:\n",
    "#                             if (re.search('state',query_english)!=None or 'where' in query_words) and (j.lower() in ['india','indian',\"india's\"]):\n",
    "#                                 possible_answers.remove(i)\n",
    "                            if (re.search('country',query_english) == None) and (j.lower() in ['india','indian',\"india's\"]):\n",
    "                                possible_answers.remove(i)\n",
    "                except:\n",
    "                    None\n",
    "            \n",
    "        possible_answers_modified_1.append(possible_answers)\n",
    "    \n",
    "    possible_answers_list=possible_answers_modified_1\n",
    "    \n",
    "    \n",
    "    best_ans=[]\n",
    "    for i in possible_answers_list:\n",
    "\n",
    "        unique_answers=[]\n",
    "        for ans in i:\n",
    "            if ans not in unique_answers:\n",
    "                unique_answers.append(ans)\n",
    "\n",
    "        unique_answers_count=[]\n",
    "        for j in unique_answers:\n",
    "            unique_answers_count.append(i.count(j))\n",
    "\n",
    "            #print(unique_answers)\n",
    "            #print(unique_answers_count)\n",
    "        if unique_answers_count==[]:\n",
    "            best_ans.append('***')\n",
    "        elif max(unique_answers_count)>1:\n",
    "            max_index=unique_answers_count.index(max(unique_answers_count))\n",
    "            best_ans.append(unique_answers[max_index])\n",
    "        else:\n",
    "            best_ans.append(i[0])    \n",
    "    print('best answer')\n",
    "    print(best_ans)\n",
    "\n",
    "    return best_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_ner(top_sentences , query_cat , trans_query , trans_query_for_checking):\n",
    "        p_list   = []\n",
    "        numb_list= []\n",
    "        loc_list = []\n",
    "        dat_list = []\n",
    "        tim_list = []\n",
    "        per_list = []\n",
    "        org_list = []\n",
    "        possible_answers = []\n",
    "        best_answers = {}\n",
    "\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        for i in range(0,len(top_sentences)):\n",
    "            ners = nlp(top_sentences[i])\n",
    "            for ent in ners.ents:\n",
    "                if(ent.label_ == \"PERSON\"):\n",
    "                    p_list.append(ent.text)\n",
    "                elif(ent.label_ == \"LOC\" or ent.label_ == \"GPE\"):\n",
    "                    loc_list.append(ent.text)\n",
    "                elif(ent.label_ == \"CARDINAL\" or ent.label_ == \"QUANTITY\"):\n",
    "                    numb_list.append(ent.text)\n",
    "                elif(ent.label_ == \"ORG\"):\n",
    "                    org_list.append(ent.text)\n",
    "                elif(ent.label_ == \"DATE\"):\n",
    "                    dat_list.append(ent.text)\n",
    "                elif(ent.label_ == \"TIME\"):\n",
    "                    tim_list.append(ent.text)\n",
    "                elif(ent.label_ == \"PERCENT\"):\n",
    "                    per_list.append(ent.text)\n",
    "\n",
    "        if query_cat == \"LOCA\":\n",
    "            possible_answers = list(set(loc_list))\n",
    "        elif query_cat == \"PERS\":\n",
    "            possible_answers = list(set(p_list))\n",
    "        elif query_cat == \"DATE\":\n",
    "            possible_answers = list(set(dat_list))\n",
    "        elif query_cat == \"ORGA\":\n",
    "            possible_answers = list(set(org_list))\n",
    "        elif query_cat == \"PERC\":\n",
    "            possible_answers = list(set(per_list))\n",
    "        elif query_cat == \"TIME\":\n",
    "            possible_answers = list(set(tim_list))\n",
    "        elif query_cat == \"NUMB\":\n",
    "            possible_answers = list(set(numb_list))\n",
    "\n",
    "        ner_sent = \" \".join(top_sentences)\n",
    "        ner_sent = ner_sent.lower()\n",
    "        for answer in possible_answers:\n",
    "            answer = answer.lower()\n",
    "            if answer not in trans_query:\n",
    "                best_answers[answer] = ner_sent.count(answer)    \n",
    "\n",
    "        #print(\"modified_ner_best_answers\", best_answers)\n",
    "        if(query_cat==\"NUMB\"):\n",
    "            best_answers = ans_preprocess(query_cat , best_answers)\n",
    "            best_answers_sorted = sorted(best_answers.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        elif(query_cat == \"PERS\"):\n",
    "\n",
    "            best_answers_sorted = sorted(best_answers.items(), key=lambda x:x[1], reverse=True)\n",
    "            best_answers = best_answer_processing_for_per(best_answers_sorted)\n",
    "            best_answers_sorted = sorted(best_answers.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        else:\n",
    "            best_answers_sorted = sorted(best_answers.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        #print(\"best answers sorted{0}\".format(best_answers_sorted))\n",
    "        if len(best_answers_sorted) == 0:\n",
    "            answer = \"UNABLE TO FIND\"\n",
    "        else:\n",
    "            if (best_answers_sorted[0][0]==\"#\"):\n",
    "                return \"#\"\n",
    "            else:\n",
    "                \n",
    "                for i in range(len(best_answers_sorted)):\n",
    "                    ans = best_answers_sorted[i][0]\n",
    "                    ans = ans.split()\n",
    "                    ans = \"\".join(ans)\n",
    "                    if len(ans) > 3: ### hard code condition\n",
    "                        if ans not in trans_query:\n",
    "                            if ans not in trans_query_for_checking:\n",
    "                                answer = ans_translation(best_answers_sorted[i][0])\n",
    "                                break\n",
    "    \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given Query :      ?\n",
      "Translated Query : In whose honor is the Nobel Prize awarded?\n",
      "query_url https://www.google.com/search?q=in+whose+honor+is+the+nobel+prize+awarded?\n",
      "Top 10 urls extracted\n",
      "https://www.nobelprize.org/ceremonies/the-nobel-prize-award-ceremonies-and-banquets/\n",
      "extract_data response code : 200\n",
      "https://en.wikipedia.org/wiki/Nobel_Prize\n",
      "extract_data response code : 200\n",
      "https://en.wikipedia.org/wiki/List_of_prizes_known_as_the_Nobel_of_a_field_or_the_highest_honors_of_a_field\n",
      "extract_data response code : 200\n",
      "https://www.nobelprize.org/prizes/lists/all-nobel-prizes\n",
      "extract_data response code : 200\n",
      "https://sweden.se/society/the-nobel-prize-awarding-great-minds/\n",
      "extract_data response code : 200\n",
      "https://sweden.se/society/the-nobel-prize/\n",
      "extract_data response code : 200\n",
      "https://www.history.com/news/7-things-you-may-not-know-about-the-nobel-prizes\n",
      "extract_data response code : 200\n",
      "https://www.harvard.edu/about-harvard/harvard-glance/honors/nobel-laureates\n",
      "extract_data response code : 200\n",
      "https://www.britannica.com/list/7-nobel-prize-scandals\n",
      "extract_data response code : 200\n",
      "https://www.investopedia.com/terms/n/nobel-memorial-prize-in-economic-sciences.asp\n",
      "extract_data response code : 200\n",
      "sent_tokenization done\n",
      "Cosine Similarity done\n",
      "Predicted Query Category : PERS\n",
      "exception in answer translation HTTPSConnectionPool(host='translate.google.com', port=443): Read timed out. (read timeout=5)\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 642\n",
      "Given Query :     ?\n",
      "Translated Query : Who built the Agra Fort?\n",
      "query_url https://www.google.com/search?q=who+built+the+agra+fort?\n",
      "Top 10 urls extracted\n",
      "https://knowindia.gov.in/culture-and-heritage/monuments/agra-fort.php\n",
      "extract_data response code : 200\n",
      "https://en.wikipedia.org/wiki/Agra_Fort\n",
      "extract_data response code : 200\n",
      "https://www.quora.com/Who-built-Agra-Fort\n",
      "extract_data response code : 200\n",
      "https://www.tutorialspoint.com/agra_fort/agra_fort_history.htm\n",
      "extract_data response code : 200\n",
      "https://www.livehistoryindia.com/amazing-india/2020/04/17/agra-fort\n",
      "extract_data response code : 200\n",
      "https://www.culturalindia.net/indian-forts/agra-fort.html\n",
      "extract_data response code : 200\n",
      "https://www.britannica.com/topic/Agra-Fort\n",
      "extract_data response code : 200\n",
      "https://www.jagranjosh.com/general-knowledge/red-fort-of-agrathe-magnum-opus-of-mughal-era-1455523150-1\n",
      "extract_data response code : 200\n",
      "sent_tokenization done\n",
      "Cosine Similarity done\n",
      "Predicted Query Category : PERS\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 643\n",
      "Given Query :   ?\n",
      "Translated Query : Who is the son of Humayun?\n",
      "query_url https://www.google.com/search?q=who+is+the+son+of+humayun?\n",
      "Top 10 urls extracted\n",
      "https://commons.wikimedia.org/wiki/File:Emperor_Humayun.JPG\n",
      "extract_data response code : 200\n",
      "https://en.wikipedia.org/wiki/Humayun\n",
      "extract_data response code : 200\n",
      "https://simple.wikipedia.org/wiki/Humayun\n",
      "extract_data response code : 200\n",
      "https://en.wikipedia.org/wiki/Kamran_Mirza\n",
      "extract_data response code : 200\n",
      "https://en.wikipedia.org/wiki/Mughal_emperors\n",
      "extract_data response code : 200\n",
      "https://www.britannica.com/biography/Humayun-Mughal-emperor\n",
      "extract_data response code : 200\n",
      "https://www.wonders-of-the-world.net/Taj-Mahal/Humayun.php\n",
      "exception HTTPSConnectionPool(host='www.wonders-of-the-world.net', port=443): Read timed out. (read timeout=5)\n",
      "https://www.wonders-of-the-world.net/Taj-Mahal/Humayun.php url max retries\n",
      "https://scroll.in/magazine/826566/how-humayun-convinced-the-love-of-his-life-to-marry-him\n",
      "extract_data response code : 200\n",
      "https://www.newworldencyclopedia.org/entry/Humayun\n",
      "exception HTTPSConnectionPool(host='www.newworldencyclopedia.org', port=443): Read timed out. (read timeout=5)\n",
      "https://www.newworldencyclopedia.org/entry/Humayun url max retries\n",
      "https://www.historydiscussion.net/history-of-india/humayun/biography-of-humayun-1530-1556-a-d-india-mughal-dynasty/6596\n",
      "extract_data response code : 403\n",
      "https://www.jagranjosh.com/general-knowledge/nasin-al-din-muammad-humayun-1441715514-1\n",
      "extract_data response code : 200\n",
      "sent_tokenization done\n",
      "Cosine Similarity done\n",
      "Predicted Query Category : PERS\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 644\n",
      "Given Query :   ( 2016 ) ?\n",
      "Translated Query : Who is the Chief Minister of Sikkim (as of November 2016)?\n",
      "query_url https://www.google.com/search?q=who+is+the+chief+minister+of+sikkim+(as+of+november+2016)?\n",
      "Top 10 urls extracted\n",
      "https://en.wikipedia.org/wiki/List_of_chief_ministers_of_Sikkim\n",
      "extract_data response code : 200\n",
      "https://en.wikipedia.org/wiki/Pawan_Kumar_Chamling\n",
      "extract_data response code : 200\n",
      "https://www.thehindu.com/elections/sikkim-assembly/who-is-ps-golay-the-new-chief-minister-of-sikkim/article27262867.ece\n",
      "extract_data response code : 200\n",
      "https://www.ndtv.com/topic/sikkim-chief-minister\n",
      "extract_data response code : 200\n",
      "https://economictimes.indiatimes.com/topic/list-of-chief-ministers-of-sikkim\n",
      "extract_data response code : 200\n",
      "https://m.economictimes.com/news/elections/lok-sabha/india/p-s-golay-sworn-in-as-sikkim-chief-minister/articleshow/69515570.cms\n",
      "exception HTTPSConnectionPool(host='economictimes.indiatimes.com', port=443): Read timed out. (read timeout=5)\n",
      "https://m.economictimes.com/news/elections/lok-sabha/india/p-s-golay-sworn-in-as-sikkim-chief-minister/articleshow/69515570.cms url max retries\n",
      "https://books.google.co.in/books?id=UmNCDgAAQBAJ\n",
      "exception HTTPSConnectionPool(host='books.google.co.in', port=443): Max retries exceeded with url: /books?id=UmNCDgAAQBAJ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000023D04EB53C8>, 'Connection to books.google.co.in timed out. (connect timeout=5)'))\n",
      "https://books.google.co.in/books?id=UmNCDgAAQBAJ url max retries\n",
      "https://books.google.co.in/books?id=zvplDwAAQBAJ\n",
      "extract_data response code : 200\n",
      "https://books.google.co.in/books?id=GAG8DwAAQBAJ\n",
      "extract_data response code : 200\n",
      "https://books.google.co.in/books?id=Go1QDwAAQBAJ\n",
      "extract_data response code : 200\n",
      "sent_tokenization done\n",
      "Cosine Similarity done\n",
      "Predicted Query Category : PERS\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 645\n",
      "Given Query : \"  \" -  ?\n",
      "Translated Query : \"Alice in Wonderland\" - author of the book?\n",
      "query_url https://www.google.com/search?q=\"alice+in+wonderland\"+-+author+of+the+book?\n"
     ]
    }
   ],
   "source": [
    "# s_engine = input(\"Enter search engine name (google/bing): \")\n",
    "s_engine = \"google\"\n",
    "search_engine = 'www.' + s_engine.lower() + '.com'\n",
    "no_of_urls = 10\n",
    "no_of_relevent_sentences = 40\n",
    "\n",
    "\n",
    "pred_answers = []\n",
    "pred_que_cat = []\n",
    "final_list = []\n",
    "count = 641\n",
    "for i  in range(641,742):\n",
    "    query = questions[i]\n",
    "    print(\"Given Query : {0}\".format(query))\n",
    "    trans_query = query_translation(query)\n",
    "    if(trans_query!=\"fail\" and trans_query!=\"None\"):\n",
    "        print(\"Translated Query : {0}\".format(trans_query))\n",
    "        query_url = query_url_generation(trans_query, search_engine)\n",
    "        print(\"query_url\",query_url)\n",
    "        url_list = g_search(trans_query,no_of_urls)\n",
    "        print(\"Top 10 urls extracted\")\n",
    "        full_text = extract_data(url_list)\n",
    "        sentences_toke = sent_tokenization(full_text)\n",
    "        print(\"sent_tokenization done\")\n",
    "\n",
    "        ############ Cosine Similarity ##########\n",
    "        similarity_dict = {}\n",
    "        cap_sim_dict = {}\n",
    "        \n",
    "        ## Removing stop words in query\n",
    "        trans_query = trans_query.lower()\n",
    "        trans_query = trans_query.replace(\"?\",\"\")\n",
    "        translated_query = trans_query\n",
    "        trans_query_list = trans_query.split()\n",
    "        trans_query_list =  [each.lower() for each in trans_query_list if each not in stopwords_list ]\n",
    "        trans_query = \" \".join(trans_query_list)\n",
    "        \n",
    "        t2 = sent_preprocess(translated_query)\n",
    "        t2_list = t2.split()\n",
    "        trans_query_for_checking = \"\".join(t2_list)\n",
    "        vect1 = from_text_to_vec(trans_query)\n",
    "        for sente in sentences_toke:\n",
    "            sente = decontracted(sent_preprocess(sente))\n",
    "            cap_sente = sente\n",
    "            sente = str(sente).lower()\n",
    "            vect2 = from_text_to_vec(sente)\n",
    "            cos_s = get_cosine(vect1, vect2)\n",
    "            similarity_dict[sente] = cos_s\n",
    "            cap_sim_dict[cap_sente] = cos_s\n",
    "        print(\"Cosine Similarity done\")\n",
    "        \n",
    "        ############ Top k Sentences  ###########\n",
    "        # Output printing\n",
    "        top_40_sentences = []\n",
    "        sorted_dict = sorted(cap_sim_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "        for i in sorted_dict[0:no_of_relevent_sentences]:\n",
    "            top_40_sentences.append(i[0])\n",
    "        top_30_sentences = top_40_sentences[:30]\n",
    "        top_20_sentences = top_40_sentences[:20]\n",
    "        top_10_sentences = top_40_sentences[:10]\n",
    "        \n",
    "        ###### Query Category starts  #######\n",
    "        query_C = Query_category(query)\n",
    "        query_cat = query_C[0]\n",
    "        print(\"Predicted Query Category : {0}\".format(query_cat))\n",
    "\n",
    "        \n",
    "        #####  Named Entity Recognition on Top k-Sentences starts #######\n",
    "        pred_answer_k10 = ner_final(top_10_sentences , query_cat, trans_query , trans_query_for_checking)\n",
    "        pred_answer_k20 = ner_final(top_20_sentences,  query_cat, trans_query , trans_query_for_checking)\n",
    "        pred_answer_k30 = ner_final(top_30_sentences,  query_cat, trans_query , trans_query_for_checking)\n",
    "        pred_answer_k40 = ner_final(top_40_sentences,  query_cat, trans_query , trans_query_for_checking)\n",
    "        \n",
    "        k10_with_mod_ner = modified_ner(top_10_sentences , query_cat, trans_query , trans_query_for_checking)\n",
    "        k20_with_mod_ner = modified_ner(top_20_sentences , query_cat, trans_query , trans_query_for_checking)\n",
    "        k30_with_mod_ner = modified_ner(top_30_sentences , query_cat, trans_query , trans_query_for_checking)\n",
    "        k40_with_mod_ner = modified_ner(top_40_sentences , query_cat, trans_query , trans_query_for_checking)\n",
    "        \n",
    "        ##### writing the data into dataframe #####\n",
    "        \n",
    "        data_obj = {\n",
    "            \"Questions\": query,\n",
    "            \"Predicted_Que_Category\":query_cat,\n",
    "            \"top_40_sentences\" : list(top_40_sentences),\n",
    "            \"Actual_Answer\":answers[count],\n",
    "            \"Predicted_Answer_k10\":pred_answer_k10,\n",
    "            \"Predicted_Answer_k20\":pred_answer_k20,\n",
    "            \"Predicted_Answer_k30\":pred_answer_k30,\n",
    "            \"Predicted_Answer_k40\":pred_answer_k40,\n",
    "            \"k10_mod_ner\":k10_with_mod_ner,\n",
    "            \"k20_mod_ner\":k20_with_mod_ner,\n",
    "            \"k30_mod_ner\":k30_with_mod_ner,\n",
    "            \"k40_mod_ner\":k40_with_mod_ner\n",
    "        }\n",
    "        final_list.append(data_obj)\n",
    "        df = pd.DataFrame(data=final_list)\n",
    "        df.to_excel(\"person-checking.xlsx\")\n",
    "        count+=1\n",
    "        print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ {0}\".format(count))\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculating Accuracies : </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"total_data_with_modified_sents.csv\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(df , col_ind1, col_ind2 , type_of_match):\n",
    "    if(type_of_match==\"exact\"):\n",
    "        pred_correct = 0\n",
    "        for i in range(0,len(df)):\n",
    "            act_val = str(df.iloc[i,col_ind1]).strip()\n",
    "            pred_val = str(df.iloc[i,col_ind2]).strip()\n",
    "            if ( act_val == pred_val ):\n",
    "                #print({act_val:pred_val})\n",
    "                pred_correct+=1\n",
    "        accuracy = (pred_correct/len(df))*100\n",
    "        return {pred_correct:accuracy}\n",
    "    elif(type_of_match==\"partial\"):\n",
    "        p_count = 0\n",
    "        for i in range(0,len(df)):\n",
    "            act_val = str(df.iloc[i,col_ind1]).strip()\n",
    "            pred_val = str(df.iloc[i,col_ind2]).strip()\n",
    "            cos_sim = get_cosine(from_text_to_vec(act_val),from_text_to_vec(pred_val))\n",
    "            if(cos_sim > 0.7):\n",
    "                #print({act_val:pred_val})\n",
    "                p_count+=1\n",
    "        p_accuracy = (p_count/len(df))*100\n",
    "        return {p_count:p_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++ k10_accuracies +++++++++++++++++++++++++++++++++\n",
      "{71: 7.618025751072961}\n",
      "{123: 13.197424892703863}\n",
      "+++++++++++++++++++++++++ k20_accuracies +++++++++++++++++++++++++++++++++\n",
      "{60: 6.437768240343347}\n",
      "{124: 13.304721030042918}\n",
      "+++++++++++++++++++++++++ k30_accuracies +++++++++++++++++++++++++++++++++\n",
      "{56: 6.008583690987124}\n",
      "{131: 14.05579399141631}\n",
      "+++++++++++++++++++++++++ k40_accuracies +++++++++++++++++++++++++++++++++\n",
      "{58: 6.223175965665236}\n",
      "{132: 14.163090128755366}\n"
     ]
    }
   ],
   "source": [
    "print(\"+++++++++++++++++++++++++ k10_accuracies +++++++++++++++++++++++++++++++++\")\n",
    "k10_accuracy = cal_accuracy(df , 5, 6,\"exact\")\n",
    "print(k10_accuracy)\n",
    "k10_accuracy_par = cal_accuracy(df, 5,6,\"partial\")\n",
    "print(k10_accuracy_par)\n",
    "\n",
    "print(\"+++++++++++++++++++++++++ k20_accuracies +++++++++++++++++++++++++++++++++\")\n",
    "k20_accuracy = cal_accuracy(df , 5 , 7,\"exact\")\n",
    "print(k20_accuracy)\n",
    "k20_accuracy_par = cal_accuracy(df, 5,7,\"partial\")\n",
    "print(k20_accuracy_par)\n",
    "\n",
    "print(\"+++++++++++++++++++++++++ k30_accuracies +++++++++++++++++++++++++++++++++\")\n",
    "k30_accuracy = cal_accuracy(df , 5, 8,\"exact\")\n",
    "print(k30_accuracy)\n",
    "k30_accuracy_par = cal_accuracy(df , 5 ,8,\"partial\")\n",
    "print(k30_accuracy_par)\n",
    "\n",
    "print(\"+++++++++++++++++++++++++ k40_accuracies +++++++++++++++++++++++++++++++++\")\n",
    "k40_accuracy = cal_accuracy(df , 5, 9,\"exact\")\n",
    "print(k40_accuracy)\n",
    "k40_accuracy_par = cal_accuracy(df , 5, 9,\"partial\")\n",
    "print(k40_accuracy_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------------+---------------------------+\n",
      "| Number of sentences |       Exact match       |       Partial match       |\n",
      "+---------------------+-------------------------+---------------------------+\n",
      "|    k10_sentences    | {71: 7.618025751072961} | {123: 13.197424892703863} |\n",
      "|    k20_sentences    | {60: 6.437768240343347} | {124: 13.304721030042918} |\n",
      "|    k30_sentences    | {56: 6.008583690987124} |  {131: 14.05579399141631} |\n",
      "|    k40_sentences    | {58: 6.223175965665236} | {132: 14.163090128755366} |\n",
      "+---------------------+-------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "tab = PrettyTable()\n",
    "tab.field_names = [\"Number of sentences\",\"Exact match\",\"Partial match\"]\n",
    "tab.add_row([\"k10_sentences\",k10_accuracy,k10_accuracy_par])\n",
    "tab.add_row([\"k20_sentences\",k20_accuracy,k20_accuracy_par])\n",
    "tab.add_row([\"k30_sentences\",k30_accuracy,k30_accuracy_par])\n",
    "tab.add_row([\"k40_sentences\",k40_accuracy,k40_accuracy_par])\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Calculating Accuracies Category Wise:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df)\n",
    "numb_cat = df[df[\"Actual_category\"]==\"NUMB\"]\n",
    "tim_cat = df[df[\"Actual_category\"] == \"TIME\"]\n",
    "dat_cat = df[df[\"Actual_category\"] == \"DATE\"]\n",
    "loc_cat = df[df[\"Actual_category\"] == \"LOCA\"]\n",
    "per_cat = df[df[\"Actual_category\"] == \"PERS\"]\n",
    "print(\"No of samples in NUMBER category  : {0}\".format(len(numb_cat)))\n",
    "print(\"No of samples in TIME category    : {0}\".format(len(tim_cat)))\n",
    "print(\"No of samples in DATE category    : {0}\".format(len(dat_cat)))\n",
    "print(\"No of samples in LOCATION category: {0}\".format(len(loc_cat)))\n",
    "print(\"No of samples in PERSON category  : {0}\".format(len(per_cat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+++++++++++++++++++++++++ k40_accuracies +++++++++++++++++++++++++++++++++\")\n",
    "k40_accuracy_numb = cal_accuracy(numb_cat , 3, 8,\"exact\")\n",
    "print(k40_accuracy_numb)\n",
    "print(\"************\")\n",
    "k40_accuracy_par_numb = cal_accuracy(numb_cat , 3, 8,\"partial\")\n",
    "print(k40_accuracy_par_numb)\n",
    "\n",
    "print(\"++++++++++++++++++++++++++ Time category ++++++++++++++++++++++++++++++++++\")\n",
    "k40_accuracy_tim = cal_accuracy(tim_cat , 3, 8 , \"exact\")\n",
    "print(k40_accuracy_tim)\n",
    "print(\"***************\")\n",
    "k40_accuracy_par_tim = cal_accuracy(tim_cat , 3, 8 , \"partial\")\n",
    "print(k40_accuracy_par_tim)\n",
    "\n",
    "print(\"++++++++++++++++++++++++++ Date category +++++++++++++++++++++++++++++++++++\")\n",
    "k40_accuracy_dat = cal_accuracy(dat_cat , 3, 8 , \"exact\")\n",
    "print(k40_accuracy_dat)\n",
    "print(\"***************\")\n",
    "k40_accuracy_par_dat = cal_accuracy(dat_cat , 3, 8 , \"partial\")\n",
    "print(k40_accuracy_par_dat)\n",
    "\n",
    "print(\"++++++++++++++++++++++++++ Location Category ++++++++++++++++++++++++++++++++\")\n",
    "k40_accuracy_loc = cal_accuracy(loc_cat , 3, 8 , \"exact\")\n",
    "print(k40_accuracy_loc)\n",
    "print(\"***************\")\n",
    "k40_accuracy_par_loc = cal_accuracy(loc_cat , 3, 8 , \"partial\")\n",
    "print(k40_accuracy_par_loc)\n",
    "\n",
    "print(\"++++++++++++++++++++++++++ Person Category +++++++++++++++++++++++++++++++++++\")\n",
    "k40_accuracy_per = cal_accuracy(per_cat , 3, 8 , \"exact\")\n",
    "print(k40_accuracy_per)\n",
    "print(\"***************\")\n",
    "k40_accuracy_par_per = cal_accuracy(per_cat , 3, 8 , \"partial\")\n",
    "print(k40_accuracy_par_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "tab = PrettyTable()\n",
    "tab.field_names = [\"Category\",\"Number of samples\",\"Exact match\",\"Partial match\"]\n",
    "tab.add_row([\"Number\",len(numb_cat),\"9.4674 %\",\"10.0591 %\"])\n",
    "tab.add_row([\"Time \",len(tim_cat),\"5.8823 %\", \"17.6470 %\"])\n",
    "tab.add_row([\"Date \",len(dat_cat),\"9.4827 %\", \"19.8275 %\"])\n",
    "tab.add_row([\"Location\",len(loc_cat),\"2.9498 %\",\"8.5545 %\"])\n",
    "tab.add_row([\"Person\",len(per_cat),\"5.4982 %\",\"15.8075 %\"])\n",
    "print(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Test Section </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = [('Netaji', 11), ('Chandra', 9), ('Bose', 8), ('Subhas Chandra', 5), ('Subhash Chandra Bose', 3), ('Gandhi', 2), ('Mamata', 1), ('Jai Hind', 1), ('Netaji Subhas Chandra Bose', 1)]\n",
    "t2 = [('Election Commissioner', 20), ('the Lok Sabha', 2), ('Sunil Arora', 2), ('Ashok Lavasa', 1), ('Sushil Chandra', 1), ('Shri Sunil Arora', 1), ('Shri Om Prakash Rawat', 1)]\n",
    "t3 = [('Sikh', 35), ('Guru', 30), ('Punjab', 8), ('Guru Nanak', 3), ('Bhakti', 3), ('Sikh Guru', 2), ('Guru Arjan', 2), ('Jassa Singh Ahluwalia', 2), ('Guru Amar Das', 2), ('Amar Das', 2), ('Ramdaspur', 1), ('Guru Hargobind', 1), ('Akal Takht', 1), ('Guru Har Gobind', 1), ('Bibi Bhani', 1), ('Guru Tegh Bahadur', 1), ('Mir Mannu', 1), ('Guru Har Rai', 1), ('Banda Singh Bahadur', 1), ('Samad Khan,[62', 1), ('Sultanul Kaum', 1), ('Guru Nanak Dev', 1), ('Mehta Kalu', 1), ('Mata Tripta', 1), ('Bhai Pratap Singh', 1), ('Bhai Karam Singh', 1), ('Chenab', 1), ('Gurdwara Bala Sahib', 1), ('Har Krishan', 1), ('Gurudwara', 1), ('Gurmat', 1), ('Guru Gobind Singh', 1), ('Guru Granth Sahib', 1)]\n",
    "t4 = [('Tendulkar', 16), ('Sachin', 5), ('Sachin Tendulkar', 3), ('Bradman', 2), (\"Donald Bradman's\", 1), ('Virat Kohli', 1), ('Steve Waugh', 1), ('MS Dhoni', 1)]\n",
    "t5 = [('Bula', 8), ('Choudhary', 1), ('Padma Shri', 1), ('Tenzing Norgay', 1), ('Bula Chowdhury', 1)]\n",
    " # do contraction: actual answer is Raja ram mohan roy\n",
    "t6 = [('Keshub', 7), ('Debendranath', 5), ('Raja Ram Mohan Roy', 3), ('Raja Rammohan Roy', 3), ('Rabindranath Tagore', 3), ('Raja Ram Mohan', 3), ('Kulin', 2), ('Brahmin', 2), ('Keshub Chandra Sen', 2), ('Feringhee Kamal Bose', 1), ('Debendranath Tagore', 1), ('Keshav Chandra Sen', 1), ('the Tattwabodhini Sabha', 1), (\"Keshub Chandra Sen's\", 1), ('Kulin Brahmanism', 1), ('Roy s Brahmo Samaj', 1), ('Swami Vivekananda', 1), (\"Keshub Chunder Sen's\", 1), ('Sudin Chattopadhyay', 1)]\n",
    " ## observe why \"ex\" is identified as person . actual answer was Shaktikanta das\n",
    "t7 = [('Ex', 5), ('Shaktikanta Das', 3), ('Osborne Smith', 2), ('Andhra Pradesh', 2), ('Deshmukh', 1), ('Shri Shaktikanta Das', 1), ('Shashikant Das', 1), ('Chakravarthi Rangarajan', 1), ('Urijit Patel', 1), ('Rakesh Mohan', 1), ('Benegal Rama Rao', 1)]\n",
    "t8 = [('Swaraj', 3), ('Pingali Venkayya', 3), ('Surayya Tyabji', 3), ('Chakhra', 3), ('Vinayak Damodar Savarkar', 2), ('Shyamji Krishna Verma', 2), ('Damodar Savarkar', 2), ('Pingali Venkaiyya', 2), ('Naveen Jindal', 1), ('Tyabjis', 1), ('Mohandas Karamchand Gandhi', 1)]\n",
    " ### Actual answer : Jithendra kumar maheswari\n",
    "t9 = [('Andhra Pradesh', 29), ('Kumar', 5), ('Maheshwari', 4), ('Jitendra Kumar Maheswari', 2), ('Biswa Bhusan', 2), ('Allahabad', 2), ('Biswa Bhusan Harichandan', 2), ('Ravi Shanker Jha', 1), ('Punjab', 1), ('Haryana HC', 1), ('Indrajit Mohanty', 1), ('JK Maheshwari', 1), ('Ranjan Gogoi', 1), ('YS Jagan', 1), ('Jul 08', 1), ('Gurgaon', 1), ('Patna', 1), ('Ranchi', 1), ('Vikram Nath', 1), ('Arup K Goswami', 1), ('Ajai Lamba', 1), ('LV Subrahmanyam', 1), ('Dipankar Datta', 1), ('Jitendra Kumar Maheshwari', 1), ('J K Maheswari', 1), ('Y.  ', 1)]\n",
    " ### Actual answer : swami dayanand saraswati\n",
    "t10 = [('Arya Samaj', 24), ('Sikh', 2), ('Dayanand Saraswati', 2), ('Dayananda Saraswati', 2), ('Nizam', 2), ('Mirza Ghulam Ahmad', 1), ('Puranas', 1), ('Quran', 1), ('Prarthana Samaj', 1), ('Rashtriya Swayamsevak Sangh', 1), ('Keshav Rao Koratkar', 1), ('Singh Sabha', 1), ('Mool Shankar', 1), ('Ajmer', 1), (\"Dayanand Saraswati's\", 1), ('Satyarth Prakash', 1), ('Satyagraha', 1), ('Swami Dayananda Saraswati', 1)]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
