{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('train_data.txt',encoding='utf-8')                     # reading the training data\n",
    "train_data=f.read()\n",
    "train_data=train_data.split('\\n')\n",
    "f.close()\n",
    "\n",
    "X=[]                                                          # separating training data into X_train and Y_train\n",
    "Y=[]\n",
    "for text in train_data:\n",
    "    Y.append(text.split(': ')[0])\n",
    "    if len(text.split(': '))==2:\n",
    "        X.append(text.split(': ')[1])\n",
    "    else:\n",
    "        X.append(':'.join(text.split(': ')[1:]))\n",
    "        \n",
    "\n",
    "from sklearn.model_selection import train_test_split          # splitting the data into train and test data\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42,stratify=Y)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   # converting text to vector using Tf-Idf\n",
    "vectorizer = TfidfVectorizer()                         \n",
    "vectorizer=vectorizer.fit(X_train)                     \n",
    "X_train_vec=vectorizer.transform(X_train).toarray() \n",
    "X_test_vec=vectorizer.transform(X_test).toarray()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder                \n",
    "encoder=LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "Y_train_vec=encoder.transform(Y_train)\n",
    "Y_test_vec=encoder.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasanth/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "          tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV                      # finding hyper parameter C=(1/lambda)\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(loss=\"squared_hinge\",multi_class=\"crammer_singer\")\n",
    "model.fit(X_train_vec, Y_train_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('test_data.txt',encoding='utf-8')                        # reading the test data\n",
    "test_data=f.read()\n",
    "test_data=test_data.split('\\n')\n",
    "f.close()\n",
    "\n",
    "questions=[]                                                     # separating test data into question and answer\n",
    "answers=[]\n",
    "for text in test_data:\n",
    "    answers.append(text.split(':::')[0])\n",
    "    questions.append(text.split(':::')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('english_questions.txt',encoding='utf-8')        # this file contains translated questions\n",
    "eng_data=f.read()\n",
    "eng_data=eng_data.split('\\n')                           # NOTE: to avoid errors during translation first i am translating all                                                       \n",
    "f.close()                                               #       telugu questions to english and stored them in a file\n",
    "eng_questions=[]                                        \n",
    "for text in eng_data:\n",
    "#     eng_questions.append(text)\n",
    "    eng_questions.append(text.lower())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_text_to_vec(text):\n",
    "    WORD = re.compile(r\"\\w+\")\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine(text1, text2):\n",
    "    vect1 = from_text_to_vec(text1)\n",
    "    vect2 = from_text_to_vec(text2)\n",
    "    intersection = set(vect1.keys()) & set(vect2.keys())\n",
    "    nume = sum([vect1[x] * vect2[x] for x in intersection])\n",
    "    \n",
    "    sum1 = sum([vect1[x]**2 for x in vect1.keys()])\n",
    "    sum2 = sum([vect2[x]**2 for x in vect2.keys()])\n",
    "    denom = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    \n",
    "    if not denom:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return nume/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_charcterwise(word1,word2):                 ### computing cosine similarity between search query & possible answers\n",
    "        w1_len = len(word1)\n",
    "        w2_len = len(word2)\n",
    "        match = 0\n",
    "\n",
    "        if w1_len > w2_len:\n",
    "            for i in range(len(word2)):\n",
    "                if word1[i] == word2[i]:\n",
    "                    match += 1\n",
    "            sim_char = match/w1_len\n",
    "        elif w2_len > w1_len:\n",
    "            for i in range(len(word1)):\n",
    "                if word2[i] == word1[i]:\n",
    "                    match += 1\n",
    "            sim_char = match/w2_len\n",
    "        else:\n",
    "            sim_char = 1\n",
    "        return sim_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webscraping(search_engine,query_telugu,query_english,sentences):\n",
    "    \n",
    "    print('entered search query: ',query_telugu)\n",
    "    print('english query: ',query_english)\n",
    "    print(sentences)\n",
    "    search_engine=search_engine.lower()\n",
    "    if search_engine=='google':                                       \n",
    "        search_query='https://www.google.dz/search?q='+query_english\n",
    "    elif search_engine=='bing':\n",
    "        search_query='https://www.bing.com/search?q='+query_english         \n",
    "    else:\n",
    "        print('chose correct search engine')\n",
    "        return \n",
    "    \n",
    "    \n",
    "    if search_engine=='google':\n",
    "        \n",
    "        from bs4 import BeautifulSoup\n",
    "        import requests\n",
    "        headers = {'user-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'}\n",
    "        page=requests.get(search_query,headers=headers)                \n",
    "        \n",
    "        soup =BeautifulSoup(page.content)              \n",
    "        \n",
    "        processed_links_google=[]                     \n",
    "        links=soup.find_all(\"div\",\"r\")\n",
    "        for link in links:\n",
    "            a_tag=link.find(\"a\")\n",
    "            processed_links_google.append(a_tag[\"href\"])\n",
    "        #print(processed_links_google)\n",
    "    \n",
    "        f=open('scrapped_google.txt','w+',encoding='utf-8')     ### now extracting text from urls\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        for i in tqdm(processed_links_google):\n",
    "            \n",
    "            try:\n",
    "        \n",
    "                page=requests.get(i,headers=headers,timeout=20)\n",
    "                if page.status_code!=200:\n",
    "                    continue\n",
    "                soup =BeautifulSoup(page.content)\n",
    "                for q in soup.find_all('p'):\n",
    "                    matched_text=q.get_text()\n",
    "                    f.write(matched_text)\n",
    "                    f.write(' ')\n",
    "            except:\n",
    "                f.write(' ')\n",
    "            \n",
    "        f.close()\n",
    "        \n",
    "    ###################### TEXT PROCESSING #########################\n",
    "         \n",
    "            \n",
    "    f2=open('scrapped_'+search_engine+'.txt','r',encoding='utf-8')\n",
    "    a=f2.read()\n",
    "    f2.close()\n",
    "    cleaned_a=''.join(i for i in a if ord(i)<128)   ### removing characters that are not in ASCII format (0-127 numbers)\n",
    "\n",
    "\n",
    "    from nltk.tokenize import sent_tokenize         ### sentence tokenization\n",
    "\n",
    "    sentences=list(sent_tokenize(cleaned_a))\n",
    "    #print(sentences)\n",
    "\n",
    "        \n",
    "    ##################### COMPUTING COS-SIM & PRINTING TOP SENTENCES #########################\n",
    "        \n",
    "        \n",
    "   \n",
    "\n",
    "    import numpy as np\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    def cosine_sim(sent1,sent2):                 ### computing cosine similarity between query & extracted sentences\n",
    "        sent1_words=word_tokenize(sent1)\n",
    "        sent2_words=word_tokenize(sent2)\n",
    "        total_words=sent1_words+sent2_words\n",
    "        unique_words=list(set(total_words))\n",
    "\n",
    "        sent1_bow=[]\n",
    "        sent2_bow=[]\n",
    "        for i in unique_words:\n",
    "            if i in sent1_words:\n",
    "                sent1_bow.append(1)\n",
    "            else:\n",
    "                sent1_bow.append(0)\n",
    "\n",
    "            if i in sent2_words:\n",
    "                sent2_bow.append(1)\n",
    "            else:\n",
    "                sent2_bow.append(0)\n",
    "\n",
    "\n",
    "        a=np.array(sent1_bow)\n",
    "        b=np.array(sent2_bow)\n",
    "        numerator=sum(a*b)\n",
    "        denominator=((sum(a**2))*(sum(b**2)))**0.5\n",
    "        cosine_similarity=numerator/denominator\n",
    "        return cosine_similarity\n",
    "\n",
    "    sim_values=[]                                  ### storing similarity values in a list\n",
    "    for sent in sentences:\n",
    "        u=cosine_sim(query_english,sent)\n",
    "        sim_values.append(u)\n",
    "    \n",
    "    \n",
    "    #print(sim_values)\n",
    "        \n",
    "    import pandas as pd                            ### creating pandas dataframe to visualize the results\n",
    "    d={'sentence':sentences,'cos-sim':sim_values}\n",
    "    output=pd.DataFrame(d)\n",
    "    #print(output)\n",
    "    output.sort_values(by=['cos-sim'],inplace=True,ascending=False)\n",
    "    \n",
    "    top_sentences=[]\n",
    "    sent_count=[10,20,30,40]\n",
    "    for k in sent_count:\n",
    "#         top_sents = sentences[:k]\n",
    "        top_sent=min(len(sentences),k)\n",
    "        top_sents=output.sentence.head(top_sent).values\n",
    "        top_sentences.append(top_sents)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###################### Question Classification #########################\n",
    "    \n",
    "    \n",
    "    \n",
    "    questions_vec=vectorizer.transform([query_telugu]).toarray()          # converting questions to vector using Tf-Idf\n",
    "\n",
    "    predicted_qc_svm=model.predict(questions_vec)\n",
    "    quest_class=encoder.inverse_transform(predicted_qc_svm)[0]\n",
    "    pred_class_list.append(quest_class)\n",
    "    print(quest_class)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###################### NER and Best-answer extraction #########################\n",
    "    \n",
    "    \n",
    "    \n",
    "    import re\n",
    "    import spacy\n",
    "    def ner_spacy(input_text):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        ners = nlp(input_text)\n",
    "\n",
    "        L=[]\n",
    "        for ent in ners.ents:\n",
    "            L.append((ent.text,ent.label_))\n",
    "\n",
    "        unique_ners = [\"PERSON\",\"NORP\",\"FAC\",\"ORG\",\"GPE\",\"LOC\",\"PRODUCT\",\"EVENT\",\"WORK_OF_ART\",\"LAW\",\"LANGUAGE\",\"DATE\",\"TIME\",\"PERCENT\",\"MONEY\",\"QUANTITY\",\"ORDINAL\",\"CARDINAL\"]\n",
    "        ners={}\n",
    "        for i in unique_ners:\n",
    "            l = []\n",
    "            for j in L:\n",
    "                if (j[1] == i):\n",
    "                    l.append(j[0])\n",
    "\n",
    "            ners[str(i)] = l\n",
    "\n",
    "        return ners\n",
    "\n",
    "\n",
    "    possible_ners=[]\n",
    "    for entry in top_sentences:\n",
    "        ner_sent=''\n",
    "        for i in entry:\n",
    "            ner_sent=ner_sent+i+''\n",
    "        a=ner_spacy(ner_sent)\n",
    "        possible_ners.append(a)\n",
    "\n",
    "    possible_answers_list=[]\n",
    "    \n",
    "    for i in range(len(sent_count)):\n",
    "        answers_extracted = []\n",
    "        if quest_class == \"LOCA\":\n",
    "            answers_extracted = possible_ners[i][\"GPE\"] + possible_ners[i][\"LOC\"]\n",
    "        elif quest_class == \"PERS\":\n",
    "            answers_extracted = possible_ners[i][\"PERSON\"]\n",
    "        elif quest_class == \"DATE\":\n",
    "            answers_extracted = possible_ners[i][\"DATE\"]\n",
    "        elif quest_class == \"ORGA\":\n",
    "            answers_extracted = possible_ners[i][\"ORG\"]\n",
    "        elif quest_class == \"PERC\":\n",
    "            answers_extracted = possible_ners[i][\"PERCENT\"]\n",
    "        elif quest_class == \"TIME\":\n",
    "            answers_extracted = possible_ners[i][\"TIME\"]\n",
    "        elif quest_class == \"NUMB\":\n",
    "            answers_extracted = possible_ners[i][\"CARDINAL\"] + possible_ners[i][\"QUANTITY\"]\n",
    "\n",
    "\n",
    "\n",
    "            ######################  #########################\n",
    "\n",
    "        modified_answers=[sub.lower().replace('one','1') for sub in answers_extracted]\n",
    "        modified_answers=[sub.lower().replace('two','2') for sub in modified_answers]\n",
    "        modified_answers=[sub.lower().replace('three','3') for sub in modified_answers]\n",
    "        modified_answers=[sub.lower().replace('four','4') for sub in modified_answers]\n",
    "        modified_answers=[sub.lower().replace('five','5') for sub in modified_answers]\n",
    "        modified_answers=[sub.lower().replace('six','6') for sub in modified_answers]\n",
    "        modified_answers=[sub.lower().replace('seven','7') for sub in modified_answers]\n",
    "        modified_answers=[sub.lower().replace('eight','8') for sub in modified_answers]\n",
    "        modified_answers=[sub.lower().replace('nine','9') for sub in modified_answers]\n",
    "        modified_answers=[sub.lower().replace('ten','10') for sub in modified_answers]\n",
    "        answers_extracted=modified_answers\n",
    "\n",
    "        possible_answers_list.append(answers_extracted)\n",
    "    \n",
    "    ###################### removing answers that are there in the query words #########################\n",
    "    \n",
    "    def cosine_sim_answers(sent1,sent2):                 ### computing cosine similarity between search query & possible answers\n",
    "        X_list = sent1.lower().split()\n",
    "        Y_list = sent2.lower().split()\n",
    "\n",
    "        l1 =[];l2 =[] \n",
    "\n",
    "        X_set = {w for w in X_list } \n",
    "        Y_set = {w for w in Y_list } \n",
    "\n",
    "        rvector = X_set.union(Y_set) \n",
    "        for w in rvector: \n",
    "            if w in X_set: l1.append(1)\n",
    "            else: l1.append(0) \n",
    "            if w in Y_set: l2.append(1) \n",
    "            else: l2.append(0) \n",
    "        c = 0\n",
    "        for i in range(len(rvector)): \n",
    "                c+= l1[i]*l2[i] \n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "\n",
    "        return cosine\n",
    "    \n",
    "    possible_answers_modified=[]\n",
    "    for count in range(len(sent_count)):\n",
    "        possible_answers=[]\n",
    "        for i in possible_answers_list[count]:\n",
    "            try:\n",
    "                if (re.search(i.lower(),query_english)) == None:\n",
    "                    possible_answers.append(i)\n",
    "            except:\n",
    "                None\n",
    "\n",
    "            if possible_answers!=[]:\n",
    "                try:\n",
    "                    for j in possible_answers:\n",
    "                            if (re.search('state',query_english)!=None or 'where' in query_words) and (j.lower() in ['india','indian',\"india's\"]):\n",
    "                                possible_answers.remove(i)\n",
    "                except:\n",
    "                    None\n",
    "            \n",
    "        possible_answers_modified.append(possible_answers)\n",
    "    \n",
    "    possible_answers_list=possible_answers_modified\n",
    "    \n",
    "    possible_answers_modified_1=[]\n",
    "    for count in range(len(sent_count)):\n",
    "        possible_answers=[]\n",
    "        for i in possible_answers_list[count]:\n",
    "            try:\n",
    "                if (re.search(i.lower(),query_english)) == None:\n",
    "                    possible_answers.append(i)\n",
    "            except:\n",
    "                None\n",
    "\n",
    "            if possible_answers!=[]:\n",
    "                try:\n",
    "                    for j in possible_answers:\n",
    "#                             if (re.search('state',query_english)!=None or 'where' in query_words) and (j.lower() in ['india','indian',\"india's\"]):\n",
    "#                                 possible_answers.remove(i)\n",
    "                            if (re.search('country',query_english) == None) and (j.lower() in ['india','indian',\"india's\"]):\n",
    "                                possible_answers.remove(i)\n",
    "                except:\n",
    "                    None\n",
    "            \n",
    "        possible_answers_modified_1.append(possible_answers)\n",
    "    \n",
    "    possible_answers_list=possible_answers_modified_1\n",
    "    \n",
    "    possible_answers_modified_2=[]\n",
    "    for count in range(len(sent_count)):\n",
    "        possible_answers=[]\n",
    "        for i in possible_answers_list[count]:\n",
    "            try:\n",
    "                if (re.search(i.lower(),query_english)) == None:\n",
    "                    possible_answers.append(i)\n",
    "            except:\n",
    "                None\n",
    "\n",
    "            if possible_answers!=[]:\n",
    "                try:\n",
    "                    for j in possible_answers:\n",
    "                            if (quest_class == \"DATE\") and (j.lower() in ['day','a day','the day','month','a month','the month','year','a year','the year']):\n",
    "                                possible_answers.remove(i)\n",
    "                except:\n",
    "                    None\n",
    "            \n",
    "        possible_answers_modified_2.append(possible_answers)\n",
    "    \n",
    "    possible_answers_list=possible_answers_modified_2\n",
    "    \n",
    "    print(possible_answers_list)\n",
    "    print('\\n')\n",
    "                   \n",
    "        \n",
    "    \n",
    "    best_ans=[]\n",
    "    for i in possible_answers_list:\n",
    "\n",
    "        unique_answers=[]\n",
    "        for ans in i:\n",
    "            if ans not in unique_answers:\n",
    "                unique_answers.append(ans)\n",
    "\n",
    "        unique_answers_count=[]\n",
    "        for j in unique_answers:\n",
    "            unique_answers_count.append(i.count(j))\n",
    "\n",
    "            #print(unique_answers)\n",
    "            #print(unique_answers_count)\n",
    "        if unique_answers_count==[]:\n",
    "            best_ans.append('***')\n",
    "        elif max(unique_answers_count)>1:\n",
    "            max_index=unique_answers_count.index(max(unique_answers_count))\n",
    "            best_ans.append(unique_answers[max_index])\n",
    "        else:\n",
    "            best_ans.append(i[0])    \n",
    "    print('best answer')\n",
    "    print(best_ans)\n",
    "\n",
    "    return best_ans\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
